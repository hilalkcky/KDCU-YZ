# -*- coding: utf-8 -*-
"""Model1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XBbyboFjcGnL3D1tuEXAe5tmqPcnHYo7
"""

!pip install tensorflow==2.15.0
!pip install --upgrade jax jaxlib

import tensorflow as tf
print(tf.__version__)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Embedding
from keras.layers import RepeatVector
from keras.layers import TimeDistributed
from keras.callbacks import ModelCheckpoint
import pandas as pd

from keras.models import Model
from keras.layers import Input, Embedding, LSTM, Dense
import numpy as np
import tensorflow as tf

print(tf.__version__)

data=pd.read_csv("data.csv")

data["CS"][1]

from tensorflow.keras.models import load_model

model2 = load_model("JAVA-CS3.h5")

java_texts = data["Java"].astype(str).tolist()
csharp_texts = data["CS"].astype(str).tolist() #kod metinlerini listeye dönüştürür.
java_tokenizer = Tokenizer()
java_tokenizer.fit_on_texts(java_texts)


csharp_tokenizer = Tokenizer()
csharp_tokenizer.fit_on_texts(csharp_texts) #oluşturulan tokenizer ile kelimeler tokenlarına ayrılır.

java_vocab_size = len(java_tokenizer.word_index) + 1
csharp_vocab_size = len(csharp_tokenizer.word_index) + 1 #oluşturulan benzersiz token sayısı yazdırılır

encoder_input_sequences = java_tokenizer.texts_to_sequences(java_texts)
decoder_input_sequences = csharp_tokenizer.texts_to_sequences(csharp_texts) #metinler sayısal dizinlere dönüştürülür.


max_java_length = max(len(seq) for seq in encoder_input_sequences)
max_csharp_length = max(len(seq) for seq in decoder_input_sequences) #sayısallaştırılan dizinlerin maksimum uzunlukları bulunur.

encoder_input_sequences = pad_sequences(encoder_input_sequences, maxlen=max_java_length, padding='post')
decoder_input_sequences = pad_sequences(decoder_input_sequences, maxlen=max_csharp_length, padding='post') #farklı maksimum uzunlukta olan dizinler aynı uzunlukta olması için maksimum dizin uzunluğuna gelene kadar 0 ile doldurulur.

decoder_output_sequences = []
for seq in decoder_input_sequences:
    decoder_output_sequences.append(to_categorical(seq, num_classes=csharp_vocab_size))

def data_generator(encoder_input_sequences, decoder_output_sequences, batch_size):
    num_batches = len(encoder_input_sequences) // batch_size
    while True:
        for i in range(num_batches):
            start = i * batch_size
            end = (i + 1) * batch_size
            yield (encoder_input_sequences[start:end], np.array(decoder_output_sequences[start:end])) #ram çökmesine karşı döngü ile batch_size parçalara ayrılır.

from sklearn.model_selection import train_test_split

encoder_train, encoder_val, decoder_train, decoder_val = train_test_split(encoder_input_sequences, decoder_output_sequences, test_size=0.2) #veri train ve test olarak ayrılır.

batch_size = 12

train_generator = data_generator(encoder_train, decoder_train, batch_size)
val_generator = data_generator(encoder_val, decoder_val, batch_size)

"""
# Modeli tanımlama
model = Sequential()
model.add(Embedding(java_vocab_size, 128, input_length=max_java_length, mask_zero=True))
model.add(LSTM(128))
model.add(RepeatVector(max_csharp_length))
model.add(LSTM(256, return_sequences=True))
model.add(TimeDistributed(Dense(csharp_vocab_size, activation='softmax')))
model.compile(optimizer='adam', loss='categorical_crossentropy')
"""
model2.fit_generator(train_generator,
                    steps_per_epoch=len(encoder_train) // batch_size,
                    epochs=5,
                    validation_data=val_generator,
                    validation_steps=len(encoder_val) // batch_size) #model kaydedilir
from keras.models import load_model


model2.save('JAVA-CS4.h5')


from google.colab import files
files.download('JAVA-CS4.h5')

from tensorflow.keras.models import load_model

model3 = load_model("JAVA-CS4.h5") #model yüklenir

#kod girdisi alınır
input_text = "public UpdateJourneyStateResult updateJourneyState(UpdateJourneyStateRequest request) {request = beforeClientExecution(request);return executeUpdateJourneyState(request);}"

input_seq = java_tokenizer.texts_to_sequences([input_text]) #metin sayısal dizinlere dönüştürülür
input_seq = pad_sequences(input_seq, maxlen=max_java_length, padding='post') #dizin boyutları eşitlenir


predicted_sequence = model2.predict(input_seq) #girilen girdi predict edilir.

predicted_token_ids = [np.argmax(vector) for vector in predicted_sequence[0]]


predicted_text = csharp_tokenizer.sequences_to_texts([predicted_token_ids])[0] #sayısal olan dizinler metine dönüştürür

print(f"Input: {input_text}")
print(f"Predicted Output: {predicted_text}")